{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2062, 64, 64)\n",
      "(2062, 10)\n",
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "X  = np.load(\"X.npy\")\n",
    "Y = np.load(\"Y.npy\")\n",
    "X,Y = shuffle(X,Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(np.shape(X[1,:]))\n",
    "LOGDIR = r\"C:\\Users\\Vishnu\\Documents\\EngProj\\Kaggle\\sign_language\\logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions Tensorflow\n",
    "def data_input(ipt,labels,b_size,tp):\n",
    "    with tf.name_scope(\"data_prep_\"+tp):\n",
    "        dset = tf.data.Dataset.from_tensor_slices((ipt,labels))\n",
    "        dset = dset.shuffle(max(ipt.shape),reshuffle_each_iteration=True)\n",
    "        dset = dset.map(map_fun).batch(b_size)\n",
    "        itr = tf.data.Iterator.from_structure(dset.output_types,dset.output_shapes)\n",
    "        itr_init_op = itr.make_initializer(dset)\n",
    "        return itr,itr_init_op\n",
    "    \n",
    "def map_fun(ipt,labels):\n",
    "    #ipt = tf.image.per_image_standardization(ipt)\n",
    "    return [ipt,labels]\n",
    "    \n",
    "def get_weight(shape,name):\n",
    "    initial_weights = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.variable(initial_weights,name)\n",
    "\n",
    "def get_bias(shape,name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial,name)\n",
    "\n",
    "def conv_2d(ipt,size_in,size_out,k_size_w,k_size_h,pool_k_size,pool_stride_size,numb):\n",
    "    with tf.name_scope(\"Conv\"+numb):      \n",
    "        #Relu Standard Deviation\n",
    "        std = np.power(2.0/(k_size_w*k_size_h*size_in),.5)\n",
    "        print(\"conv\"+numb,std)\n",
    "        #in_w = tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev=std)\n",
    "        W = tf.Variable(tf.truncated_normal([k_size_w,k_size_h,size_in,size_out],stddev=std),name=\"W\"+numb)\n",
    "        b = tf.constant(0.1,shape=[size_out],name=\"B\"+numb)\n",
    "        \n",
    "        c = tf.nn.conv2d(ipt,W,strides=[1,1,1,1],padding=\"SAME\",name=\"Conv\"+numb)\n",
    "        act = tf.nn.leaky_relu(c+b,alpha=0.3)\n",
    "        #act = tf.nn.relu(c+b)\n",
    "        \n",
    "        tf.summary.histogram(\"Weights\",W)\n",
    "        tf.summary.histogram(\"Bias\",W)\n",
    "        tf.summary.histogram(\"Activation\",act)\n",
    "        return tf.nn.max_pool(act,ksize=[1,k_size_w,k_size_h,1],strides=[1,pool_stride_size,pool_stride_size,1],padding=\"SAME\",name=\"pool\"+numb)\n",
    "    \n",
    "def fc_layer(ipt,size_in,size_out,batch_d,numb):\n",
    "    with tf.name_scope(\"FC\"+numb):\n",
    "        #Using Batch Norm\n",
    "        #Initialize Weights\n",
    "        std = np.power(2.0/(size_in*size_out),.5)\n",
    "        print(\"FC\"+numb,std)\n",
    "        W = tf.Variable(tf.truncated_normal([size_in, size_out],stddev=std),name=\"W_FC\"+numb)\n",
    "        #b = tf.constant(0.1,shape=[size_out],name=\"B_FC\"+numb)\n",
    "        z = tf.matmul(ipt,W)\n",
    "        bt_norm = batch_norm(z,numb,batch_d)\n",
    "        \n",
    "        act = tf.nn.leaky_relu(bt_norm,alpha=.1)\n",
    "        act = tf.nn.dropout(act,tf.get_default_graph().get_tensor_by_name(\"placeholder/keep_prob:0\"))\n",
    "        \n",
    "        \n",
    "        tf.summary.histogram(\"Weights\",W)\n",
    "        tf.summary.histogram(\"Activation\",act)\n",
    "        return act\n",
    "    \n",
    "def batch_norm(ipt,numb,decay):\n",
    "    with tf.name_scope(\"Batch_norm\"+numb):\n",
    "        #Defining Batch Terms\n",
    "        b_beta = tf.Variable(tf.zeros(ipt.get_shape()[-1]),name=\"beta\"+numb)\n",
    "        b_gamma= tf.Variable(tf.ones(ipt.get_shape()[-1]),name=\"gamma\"+numb)\n",
    "        #Population Terms\n",
    "        pop_mean = tf.Variable(tf.zeros(ipt.get_shape()[-1]),trainable=False,name=\"pop_mean\"+numb)\n",
    "        pop_var = tf.Variable(tf.ones(ipt.get_shape()[-1]),trainable=False,name=\"pop_var\"+numb)\n",
    "    \n",
    "        tf.summary.histogram(\"Beta\",b_beta)\n",
    "        tf.summary.histogram(\"Gamma\",b_gamma)\n",
    "        tf.summary.histogram(\"pop_mean\",pop_mean)\n",
    "        tf.summary.histogram(\"pop_var\",pop_var)\n",
    "        \n",
    "        #If is training\n",
    "        def f_true():\n",
    "            m,var = tf.nn.moments(ipt,axes=[0],name=\"comp_mvar\"+numb)\n",
    "            t_mean = tf.assign(pop_mean,pop_mean*decay+m*(1-decay),name=\"T_mean\")\n",
    "            t_var = tf.assign(pop_var,pop_var*decay+var*(1-decay),name=\"T_var\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            with tf.control_dependencies([t_mean,t_var]):\n",
    "                return tf.nn.batch_normalization(ipt,m,var,b_beta,b_gamma,variance_epsilon=.05,name=\"bnorm\"+numb)\n",
    "        \n",
    "            \n",
    "        #If is inference\n",
    "        def f_false():\n",
    "            return tf.nn.batch_normalization(ipt,pop_mean,pop_var,b_beta,b_gamma,variance_epsilon=0.5,name=\"bnorm\"+numb)\n",
    "        \n",
    "        t = tf.get_default_graph().get_tensor_by_name(\"placeholder/is_training:0\")\n",
    "        rtn_val = tf.cond(t,f_true,f_false,name=\"Pop_or_Batch\")\n",
    "        return rtn_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model \n",
    "#def model(conv_layers,kernels,fc_layers,nodes,learning_rate):\n",
    "def model(conv_layers,fc_layers,learning_rate,lr_decay,batch_size,batch_d,epochs):\n",
    "    #Prelims\n",
    "    tf.reset_default_graph()\n",
    "    g_step = tf.train.create_global_step()\n",
    "        \n",
    "    #Seperating Data\n",
    "    tot_data = len(X)\n",
    "    T_data_len = int((2.0/3)*tot_data)\n",
    "    print(T_data_len)\n",
    "    TX = X[0:T_data_len,:]\n",
    "    TY = Y[0:T_data_len,:]\n",
    "    \n",
    "    TrX = X[T_data_len+1:,:]\n",
    "    TrY = Y[T_data_len+1:,:]\n",
    "    print(len(TX),len(TrX))\n",
    "    TX = TX[:,:,:,np.newaxis]\n",
    "    TY = TY[:,:,np.newaxis]\n",
    "    \n",
    "    TrX = TrX[:,:,:,np.newaxis]\n",
    "    TrY = TrY[:,:,np.newaxis]\n",
    "    print(np.shape(TX),np.shape(TrX))\n",
    "    #Data Iterators\n",
    "\n",
    "    T_iter, T_iter_init_op = data_input(TX,TY,batch_size,\"Train\")\n",
    "    TR_iter, TR_iter_init_op = data_input(TrX,TrY,batch_size,\"Test\")\n",
    "    \n",
    "    #Placeholder\n",
    "    with tf.name_scope(\"placeholder\"):\n",
    "        is_training = tf.placeholder(tf.bool,name=\"is_training\")\n",
    "        keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "        \n",
    "    #Variables\n",
    "    with tf.name_scope(\"Variable\"):\n",
    "        learning_rt = tf.Variable(learning_rate,name=\"learning_rate\")\n",
    "\n",
    "    with tf.name_scope(\"lr_decay\"):\n",
    "        dec_lr = tf.train.exponential_decay(learning_rt,g_step,batch_size,lr_decay,staircase=True)\n",
    "        tf.summary.scalar(\"Learning_rate\",dec_lr)\n",
    "    \n",
    "    \n",
    "    def f_true():\n",
    "        rtval = T_iter.get_next()\n",
    "        return rtval\n",
    "    \n",
    "    def f_false():\n",
    "        rtval = TR_iter.get_next()\n",
    "        return rtval\n",
    "    \n",
    "    dat = tf.cond(is_training,f_true,f_false,strict=True,name=\"Conditional\")\n",
    "    \n",
    "    img = dat[0]\n",
    "    label = dat[1]\n",
    "    img = tf.reshape(img,[-1,64,64,1])\n",
    "    label = tf.reshape(tf.cast(label,tf.int32),[-1,10])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Model Definition\n",
    "    convs = []\n",
    "    convs.append(img)\n",
    "    conv_layers[0] = 1\n",
    "    for i in range(0,len(conv_layers)-1):\n",
    "        convs.append(conv_2d(convs[i],conv_layers[i],conv_layers[i+1],5,5,5,2,str(i+1)))\n",
    "    \n",
    "    f_convs = convs[len(convs)-1]\n",
    "    f_convs_shp = f_convs.get_shape().as_list()\n",
    "    fc_layers[0] = f_convs_shp[1]*f_convs_shp[2]*f_convs_shp[3]\n",
    "    print(\"FC_LAYERS\",fc_layers[0])\n",
    "    fcs = []\n",
    "    print(\"BF reshape\",f_convs.get_shape().as_list())\n",
    "    flatten = tf.reshape(f_convs,[-1,fc_layers[0]])\n",
    "    fcs.append(flatten)\n",
    "    \n",
    "    for i in range(0,len(fc_layers)-1):\n",
    "        fcs.append(fc_layer(fcs[i],fc_layers[i],fc_layers[i+1],batch_d,str(i+1)))\n",
    "    \n",
    "    \n",
    "    es = fcs[len(fcs)-1]\n",
    "    with tf.name_scope(\"xentropy\"):\n",
    "        xtrpy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=es,labels=tf.stop_gradient(label)))\n",
    "        tf.summary.scalar(\"cross_entropy\",xtrpy)\n",
    "        \n",
    "    with tf.name_scope(\"Train\"):\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            #train = tf.train.GradientDescentOptimizer(dec_lr).minimize(xtrpy,global_step = g_step)\n",
    "            train = tf.train.AdamOptimizer(dec_lr).minimize(xtrpy,global_step = g_step)\n",
    "    \n",
    "    with tf.name_scope(\"Accuracy\"):\n",
    "        pred = tf.equal(tf.argmax(es,1),tf.argmax(label,axis=1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(pred,tf.float32))\n",
    "        tf.summary.scalar(\"acc\",accuracy)\n",
    "        \n",
    "    summ = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(LOGDIR)\n",
    "    cor = 0\n",
    "    p = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(T_iter_init_op)\n",
    "\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        for i in range(1,epochs):\n",
    "            ac = []\n",
    "            sess.run(T_iter_init_op)\n",
    "            while(True):\n",
    "                p = p+1\n",
    "                try:\n",
    "                    a,acc,s = sess.run([train,accuracy,summ],{is_training: True,keep_prob: 1.0})\n",
    "                    writer.add_summary(s,p)\n",
    "                    #print(np.argmax(ess,axis=1),np.argmax(labels,axis=1))\n",
    "                    #print(tm,tv)\n",
    "                    ac.append(acc)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print(\"Finished Training: \"+str(i))\n",
    "                    break\n",
    "            print(np.average(acc))\n",
    "            \n",
    "        writer.close()\n",
    "        \n",
    "        cor = 0\n",
    "        sess.run(T_iter_init_op)\n",
    "        while(True):\n",
    "            try:\n",
    "                ess,labels,pp = sess.run([es,label,pred],{is_training: True, keep_prob: 1.0})\n",
    "                for i in pp:\n",
    "                    if (i):\n",
    "                        cor = cor+1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Finished Infer\")\n",
    "                break;\n",
    "        print(\"Training Acc: \",float(cor)/max(TX.shape))\n",
    "        \n",
    "        \n",
    "        sess.run(TR_iter_init_op)\n",
    "        cor = 0\n",
    "        while(True):\n",
    "            try:\n",
    "                ess,labels,pp= sess.run([es,label,pred],{is_training: False,keep_prob:1.0})\n",
    "                for i in pp:\n",
    "                    if (i):\n",
    "                        cor = cor+1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Finished Infer\")\n",
    "                break\n",
    "        \n",
    "        \n",
    "        print(\"Test Acc: \",float(cor)/len(TrX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374\n",
      "1374 687\n",
      "(1374, 64, 64, 1) (687, 64, 64, 1)\n",
      "conv1 0.282842712474619\n",
      "conv2 0.1\n",
      "conv3 0.07071067811865475\n",
      "FC_LAYERS 2048\n",
      "BF reshape [None, 8, 8, 32]\n",
      "FC1 0.0009882117688026185\n",
      "FC2 0.0025819888974716113\n",
      "FC3 0.006666666666666667\n",
      "FC4 0.016329931618554522\n",
      "FC5 0.06324555320336758\n",
      "Finished Training: 1\n",
      "0.29166666\n",
      "Finished Training: 2\n",
      "0.33333334\n",
      "Finished Training: 3\n",
      "0.7083333\n",
      "Finished Training: 4\n",
      "0.6666667\n",
      "Finished Training: 5\n",
      "0.7083333\n",
      "Finished Training: 6\n",
      "0.7083333\n",
      "Finished Training: 7\n",
      "0.875\n",
      "Finished Training: 8\n",
      "0.9166667\n",
      "Finished Training: 9\n",
      "0.875\n",
      "Finished Training: 10\n",
      "0.9583333\n",
      "Finished Training: 11\n",
      "0.9583333\n",
      "Finished Training: 12\n",
      "0.9166667\n",
      "Finished Training: 13\n",
      "0.9583333\n",
      "Finished Training: 14\n",
      "1.0\n",
      "Finished Infer\n",
      "Training Acc:  0.9679767103347889\n",
      "Finished Infer\n",
      "Test Acc:  0.9737991266375546\n"
     ]
    }
   ],
   "source": [
    "conv = [0,8,16,32]\n",
    "fclyr = [0,1000,300,150,50,10]\n",
    "bs = 30\n",
    "lr = 1.0\n",
    "lrd = .9\n",
    "epochs = 15\n",
    "b_decay = .8\n",
    "model(conv,fclyr,lr,lrd,bs,b_decay,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
